{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\renan.baisso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\renan.baisso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_segmentation (string):\n",
    "    segmented_string = nltk.sent_tokenize(string)\n",
    "    return segmented_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_without_punct(string):\n",
    "    regex_punctuation = r'[^\\w\\s]'\n",
    "    string_without_punct = re.sub(regex_punctuation,'',string)\n",
    "    return string_without_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.VERB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    \n",
    "    for w in words:\n",
    "        lemmas.append(lemmatizer.lemmatize(w, get_wordnet_pos(w)))\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(string):\n",
    "    #https://www.w3schools.com/python/python_regex.asp#findall\n",
    "    regex = r\"\\b[-a-zA-ZÀ-ÖØ-öø-ÿ0-9]+\\b\"\n",
    "    return re.findall(regex, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(word):\n",
    "    return [ w.lower() for w in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PorterStemmerCustom:\n",
    "    #https://tartarus.org/martin/PorterStemmer/python.txt\n",
    "    def isCons(self, letter):\n",
    "        if letter == 'a' or letter == 'e' or letter == 'i' or letter == 'o' or letter == 'u':\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def isConsonant(self, word, i):\n",
    "        letter = word[i]\n",
    "        if self.isCons(letter):\n",
    "            if letter == 'y' and self.isCons(word[i-1]):\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def isVowel(self, word, i):\n",
    "        return not(self.isConsonant(word, i))\n",
    "\n",
    "    # *S\n",
    "    def endsWith(self, stem, letter):\n",
    "        if stem.endswith(letter):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # *v*\n",
    "    def containsVowel(self, stem):\n",
    "        for i in stem:\n",
    "            if not self.isCons(i):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # *d\n",
    "    def doubleCons(self, stem):\n",
    "        if len(stem) >= 2:\n",
    "            if self.isConsonant(stem, -1) and self.isConsonant(stem, -2):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def getForm(self, word):\n",
    "        form = []\n",
    "        formStr = ''\n",
    "        for i in range(len(word)):\n",
    "            if self.isConsonant(word, i):\n",
    "                if i != 0:\n",
    "                    prev = form[-1]\n",
    "                    if prev != 'C':\n",
    "                        form.append('C')\n",
    "                else:\n",
    "                    form.append('C')\n",
    "            else:\n",
    "                if i != 0:\n",
    "                    prev = form[-1]\n",
    "                    if prev != 'V':\n",
    "                        form.append('V')\n",
    "                else:\n",
    "                    form.append('V')\n",
    "        for j in form:\n",
    "            formStr += j\n",
    "        return formStr\n",
    "\n",
    "    def getM(self, word):\n",
    "        form = self.getForm(word)\n",
    "        m = form.count('VC')\n",
    "        return m\n",
    "\n",
    "    # *o\n",
    "    def cvc(self, word):\n",
    "        if len(word) >= 3:\n",
    "            f = -3\n",
    "            s = -2\n",
    "            t = -1\n",
    "            third = word[t]\n",
    "            if self.isConsonant(word, f) and self.isVowel(word, s) and self.isConsonant(word, t):\n",
    "                if third != 'w' and third != 'x' and third != 'y':\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def replace(self, orig, rem, rep):\n",
    "        result = orig.rfind(rem)\n",
    "        base = orig[:result]\n",
    "        replaced = base + rep\n",
    "        return replaced\n",
    "\n",
    "    def replaceM0(self, orig, rem, rep):\n",
    "        result = orig.rfind(rem)\n",
    "        base = orig[:result]\n",
    "        if self.getM(base) > 0:\n",
    "            replaced = base + rep\n",
    "            return replaced\n",
    "        else:\n",
    "            return orig\n",
    "\n",
    "    def replaceM1(self, orig, rem, rep):\n",
    "        result = orig.rfind(rem)\n",
    "        base = orig[:result]\n",
    "        if self.getM(base) > 1:\n",
    "            replaced = base + rep\n",
    "            return replaced\n",
    "        else:\n",
    "            return orig\n",
    "\n",
    "    def step1a(self, word):\n",
    "        if word.endswith('sses'):\n",
    "            word = self.replace(word, 'sses', 'ss')\n",
    "        elif word.endswith('ies'):\n",
    "            word = self.replace(word, 'ies', 'i')\n",
    "        elif word.endswith('ss'):\n",
    "            word = self.replace(word, 'ss', 'ss')\n",
    "        elif word.endswith('s'):\n",
    "            word = self.replace(word, 's', '')\n",
    "        else:\n",
    "            pass\n",
    "        return word\n",
    "\n",
    "    def step1b(self, word):\n",
    "        flag = False\n",
    "        if word.endswith('eed'):\n",
    "            result = word.rfind('eed')\n",
    "            base = word[:result]\n",
    "            if self.getM(base) > 0:\n",
    "                word = base\n",
    "                word += 'ee'\n",
    "        elif word.endswith('ed'):\n",
    "            result = word.rfind('ed')\n",
    "            base = word[:result]\n",
    "            if self.containsVowel(base):\n",
    "                word = base\n",
    "                flag = True\n",
    "        elif word.endswith('ing'):\n",
    "            result = word.rfind('ing')\n",
    "            base = word[:result]\n",
    "            if self.containsVowel(base):\n",
    "                word = base\n",
    "                flag = True\n",
    "        if flag:\n",
    "            if word.endswith('at') or word.endswith('bl') or word.endswith('iz'):\n",
    "                word += 'e'\n",
    "            elif self.doubleCons(word) and not self.endsWith(word, 'l') and not self.endsWith(word, 's') and not self.endsWith(word, 'z'):\n",
    "                word = word[:-1]\n",
    "            elif self.getM(word) == 1 and self.cvc(word):\n",
    "                word += 'e'\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "        return word\n",
    "\n",
    "    def step1c(self, word):\n",
    "        if word.endswith('y'):\n",
    "            result = word.rfind('y')\n",
    "            base = word[:result]\n",
    "            if self.containsVowel(base):\n",
    "                word = base\n",
    "                word += 'i'\n",
    "        return word\n",
    "\n",
    "    def step2(self, word):\n",
    "        if word.endswith('ational'):\n",
    "            word = self.replaceM0(word, 'ational', 'ate')\n",
    "        elif word.endswith('tional'):\n",
    "            word = self.replaceM0(word, 'tional', 'tion')\n",
    "        elif word.endswith('enci'):\n",
    "            word = self.replaceM0(word, 'enci', 'ence')\n",
    "        elif word.endswith('anci'):\n",
    "            word = self.replaceM0(word, 'anci', 'ance')\n",
    "        elif word.endswith('izer'):\n",
    "            word = self.replaceM0(word, 'izer', 'ize')\n",
    "        elif word.endswith('abli'):\n",
    "            word = self.replaceM0(word, 'abli', 'able')\n",
    "        elif word.endswith('alli'):\n",
    "            word = self.replaceM0(word, 'alli', 'al')\n",
    "        elif word.endswith('entli'):\n",
    "            word = self.replaceM0(word, 'entli', 'ent')\n",
    "        elif word.endswith('eli'):\n",
    "            word = self.replaceM0(word, 'eli', 'e')\n",
    "        elif word.endswith('ousli'):\n",
    "            word = self.replaceM0(word, 'ousli', 'ous')\n",
    "        elif word.endswith('ization'):\n",
    "            word = self.replaceM0(word, 'ization', 'ize')\n",
    "        elif word.endswith('ation'):\n",
    "            word = self.replaceM0(word, 'ation', 'ate')\n",
    "        elif word.endswith('ator'):\n",
    "            word = self.replaceM0(word, 'ator', 'ate')\n",
    "        elif word.endswith('alism'):\n",
    "            word = self.replaceM0(word, 'alism', 'al')\n",
    "        elif word.endswith('iveness'):\n",
    "            word = self.replaceM0(word, 'iveness', 'ive')\n",
    "        elif word.endswith('fulness'):\n",
    "            word = self.replaceM0(word, 'fulness', 'ful')\n",
    "        elif word.endswith('ousness'):\n",
    "            word = self.replaceM0(word, 'ousness', 'ous')\n",
    "        elif word.endswith('aliti'):\n",
    "            word = self.replaceM0(word, 'aliti', 'al')\n",
    "        elif word.endswith('iviti'):\n",
    "            word = self.replaceM0(word, 'iviti', 'ive')\n",
    "        elif word.endswith('biliti'):\n",
    "            word = self.replaceM0(word, 'biliti', 'ble')\n",
    "        return word\n",
    "\n",
    "    def step3(self, word):\n",
    "        if word.endswith('icate'):\n",
    "            word = self.replaceM0(word, 'icate', 'ic')\n",
    "        elif word.endswith('ative'):\n",
    "            word = self.replaceM0(word, 'ative', '')\n",
    "        elif word.endswith('alize'):\n",
    "            word = self.replaceM0(word, 'alize', 'al')\n",
    "        elif word.endswith('iciti'):\n",
    "            word = self.replaceM0(word, 'iciti', 'ic')\n",
    "        elif word.endswith('ful'):\n",
    "            word = self.replaceM0(word, 'ful', '')\n",
    "        elif word.endswith('ness'):\n",
    "            word = self.replaceM0(word, 'ness', '')\n",
    "        return word\n",
    "\n",
    "    def step4(self, word):\n",
    "        if word.endswith('al'):\n",
    "            word = self.replaceM1(word, 'al', '')\n",
    "        elif word.endswith('ance'):\n",
    "            word = self.replaceM1(word, 'ance', '')\n",
    "        elif word.endswith('ence'):\n",
    "            word = self.replaceM1(word, 'ence', '')\n",
    "        elif word.endswith('er'):\n",
    "            word = self.replaceM1(word, 'er', '')\n",
    "        elif word.endswith('ic'):\n",
    "            word = self.replaceM1(word, 'ic', '')\n",
    "        elif word.endswith('able'):\n",
    "            word = self.replaceM1(word, 'able', '')\n",
    "        elif word.endswith('ible'):\n",
    "            word = self.replaceM1(word, 'ible', '')\n",
    "        elif word.endswith('ant'):\n",
    "            word = self.replaceM1(word, 'ant', '')\n",
    "        elif word.endswith('ement'):\n",
    "            word = self.replaceM1(word, 'ement', '')\n",
    "        elif word.endswith('ment'):\n",
    "            word = self.replaceM1(word, 'ment', '')\n",
    "        elif word.endswith('ent'):\n",
    "            word = self.replaceM1(word, 'ent', '')\n",
    "        elif word.endswith('ou'):\n",
    "            word = self.replaceM1(word, 'ou', '')\n",
    "        elif word.endswith('ism'):\n",
    "            word = self.replaceM1(word, 'ism', '')\n",
    "        elif word.endswith('ate'):\n",
    "            word = self.replaceM1(word, 'ate', '')\n",
    "        elif word.endswith('iti'):\n",
    "            word = self.replaceM1(word, 'iti', '')\n",
    "        elif word.endswith('ous'):\n",
    "            word = self.replaceM1(word, 'ous', '')\n",
    "        elif word.endswith('ive'):\n",
    "            word = self.replaceM1(word, 'ive', '')\n",
    "        elif word.endswith('ize'):\n",
    "            word = self.replaceM1(word, 'ize', '')\n",
    "        elif word.endswith('ion'):\n",
    "            result = word.rfind('ion')\n",
    "            base = word[:result]\n",
    "            if self.getM(base) > 1 and (self.endsWith(base, 's') or self.endsWith(base, 't')):\n",
    "                word = base\n",
    "            word = self.replaceM1(word, '', '')\n",
    "        return word\n",
    "\n",
    "    def step5a(self, word):\n",
    "        if word.endswith('e'):\n",
    "            base = word[:-1]\n",
    "            if self.getM(base) > 1:\n",
    "                word = base\n",
    "            elif self.getM(base) == 1 and not self.cvc(base):\n",
    "                word = base\n",
    "        return word\n",
    "\n",
    "    def step5b(self, word):\n",
    "        if self.getM(word) > 1 and self.doubleCons(word) and self.endsWith(word, 'l'):\n",
    "            word = word[:-1]\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        word = self.step1a(word)\n",
    "        word = self.step1b(word)\n",
    "        word = self.step1c(word)\n",
    "        word = self.step2(word)\n",
    "        word = self.step3(word)\n",
    "        word = self.step4(word)\n",
    "        word = self.step5a(word)\n",
    "        word = self.step5b(word)\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemArray(words):\n",
    "    psc = PorterStemmerCustom()\n",
    "    result = []\n",
    "    for w in words:\n",
    "        result.append(psc.stem(w))\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopWordsHandler:\n",
    "    #https://stackoverflow.com/questions/6022764/python-removing-list-element-while-iterating-over-list/6024599\n",
    "    #https://gist.github.com/sebleier/554280\n",
    "    def isStopWord(self,word):\n",
    "        stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "        if word in stop_words:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def removeStopWords(self,words):\n",
    "        for i in list(words):\n",
    "            if self.isStopWord(i) or i == '':\n",
    "                words.remove(i)\n",
    "            else:\n",
    "                pass\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCorpusData(path):\n",
    "    files = []\n",
    "    read_tuples = []\n",
    "    #Lendo arquivos da pasta data/\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            #Caso seja .xls, eh o excel que possui os as respostas para o treinamento\n",
    "            if '.xls' in file:\n",
    "                xls = pd.ExcelFile(f'{path}{file}')\n",
    "                #Transformar em DataFrame\n",
    "                df = xls.parse('File list')\n",
    "            elif '.txt' in file:\n",
    "                with open(f'{path}{file}', 'r', encoding=\"utf8\", errors='ignore') as j:\n",
    "                    #Montar um lista de tuplas para mapear arquivo e conteudos\n",
    "                    read_tuples.append((file, j.read()))\n",
    "\n",
    "    #Transformar lista de tuplas em DF\n",
    "    df_tuples = pd.DataFrame(read_tuples, columns=['File', 'Content'])\n",
    "    #Realizar o Join entre os arquivos para mapear conteudo e respostas de treinamento\n",
    "    return pd.merge(df_tuples, df, how='left', left_on = 'File', right_on = 'File')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Category</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g0pA_taska.txt</td>\n",
       "      <td>non</td>\n",
       "      <td>Inheritance is a basic concept of Object-Orien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g0pA_taskb.txt</td>\n",
       "      <td>cut</td>\n",
       "      <td>PageRank is a link analysis algorithm used by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g0pA_taskc.txt</td>\n",
       "      <td>light</td>\n",
       "      <td>The vector space model (also called, term vect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g0pA_taskd.txt</td>\n",
       "      <td>heavy</td>\n",
       "      <td>Bayes’ theorem was names after Rev Thomas Baye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g0pA_taske.txt</td>\n",
       "      <td>non</td>\n",
       "      <td>Dynamic Programming is an algorithm design tec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             File Category                                            Content\n",
       "0  g0pA_taska.txt      non  Inheritance is a basic concept of Object-Orien...\n",
       "1  g0pA_taskb.txt      cut  PageRank is a link analysis algorithm used by ...\n",
       "2  g0pA_taskc.txt    light  The vector space model (also called, term vect...\n",
       "3  g0pA_taskd.txt    heavy  Bayes’ theorem was names after Rev Thomas Baye...\n",
       "4  g0pA_taske.txt      non  Dynamic Programming is an algorithm design tec..."
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'data/'\n",
    "corpusDF = readCorpusData(path)\n",
    "corpusDF = corpusDF[['File', 'Category','Content']]\n",
    "corpusDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "taska = corpusDF[corpusDF.File.str.contains('taska.txt')].fillna(0)\n",
    "taskb = corpusDF[corpusDF.File.str.contains('taskb.txt')].fillna(0)\n",
    "taskc = corpusDF[corpusDF.File.str.contains('taskc.txt')].fillna(0)\n",
    "taskd = corpusDF[corpusDF.File.str.contains('taskd.txt')].fillna(0)\n",
    "taske = corpusDF[corpusDF.File.str.contains('taske.txt')].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "taske['Tokens'] = taske['Content'].map(tokenizer).map(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCount(words):\n",
    "    count= nltk.defaultdict(int)\n",
    "    for word in words:\n",
    "        count[word] += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_dist(v1, v2):\n",
    "    import numpy as np\n",
    "    product = np.dot(v1,v2)\n",
    "    \n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    \n",
    "    return product/(norm_v2*norm_v1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSim(t1, t2):\n",
    "    import numpy as np\n",
    "    vocabulary = []\n",
    "    \n",
    "    for key in t1:\n",
    "        vocabulary.append(key)\n",
    "    for key in t2:\n",
    "        vocabulary.append(key)\n",
    "    \n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    d1 = np.zeros(vocabulary_size, dtype=np.int)\n",
    "    d2 = np.zeros(vocabulary_size, dtype=np.int)\n",
    "    \n",
    "    i = 0\n",
    "    for (k) in vocabulary:\n",
    "        d1[i] = t1.get(k, 0)\n",
    "        d2[i] = t2.get(k, 0)\n",
    "        i += 1\n",
    "\n",
    "    return cos_dist(d1, d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g0pA_taske.txt</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>g0pB_taske.txt</td>\n",
       "      <td>heavy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>g0pC_taske.txt</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>g0pD_taske.txt</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>g0pE_taske.txt</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>g1pA_taske.txt</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>g1pB_taske.txt</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>g1pD_taske.txt</td>\n",
       "      <td>heavy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>g2pA_taske.txt</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>g2pB_taske.txt</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>g2pC_taske.txt</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>g2pE_taske.txt</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>g3pA_taske.txt</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>g3pB_taske.txt</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>g3pC_taske.txt</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>g4pB_taske.txt</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>g4pC_taske.txt</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>g4pD_taske.txt</td>\n",
       "      <td>heavy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>g4pE_taske.txt</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>orig_taske.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              File Category\n",
       "4   g0pA_taske.txt      non\n",
       "9   g0pB_taske.txt    heavy\n",
       "14  g0pC_taske.txt    light\n",
       "19  g0pD_taske.txt      non\n",
       "24  g0pE_taske.txt      cut\n",
       "29  g1pA_taske.txt      non\n",
       "34  g1pB_taske.txt      cut\n",
       "39  g1pD_taske.txt    heavy\n",
       "44  g2pA_taske.txt      non\n",
       "49  g2pB_taske.txt      cut\n",
       "54  g2pC_taske.txt    light\n",
       "59  g2pE_taske.txt      non\n",
       "64  g3pA_taske.txt      non\n",
       "69  g3pB_taske.txt      cut\n",
       "74  g3pC_taske.txt    light\n",
       "79  g4pB_taske.txt      cut\n",
       "84  g4pC_taske.txt    light\n",
       "89  g4pD_taske.txt    heavy\n",
       "94  g4pE_taske.txt      non\n",
       "99  orig_taske.txt        0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taske['WCount'] = taske['Tokens'].map(wordCount)\n",
    "taske[['File','Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createF1(task):\n",
    "    task['Tokens'] = task['Content'].map(tokenizer).map(normalizer)\n",
    "    task['WCount'] = task['Tokens'].map(wordCount)\n",
    "    task['f1'] = task.WCount.map(lambda t: getSim(task.WCount.iloc[-1], t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createF2(task):\n",
    "    ps = PorterStemmerCustom()\n",
    "    task['Tokens'] = task['Content'].map(tokenizer).map(normalizer)\n",
    "    swh = StopWordsHandler()\n",
    "    task['StopWordsOut'] = task['Tokens'].map(swh.removeStopWords)\n",
    "    task['Stem'] = task['StopWordsOut'].map(stemArray)\n",
    "    task['WCount'] = task['Stem'].map(wordCount)\n",
    "    task['f2'] = task.WCount.map(lambda t: getSim(task.WCount.iloc[-1], t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createF3(task):\n",
    "    ps = PorterStemmerCustom()\n",
    "    task['Tokens'] = task['Content'].map(tokenizer).map(normalizer)\n",
    "    swh = StopWordsHandler()\n",
    "    task['StopWordsOut'] = task['Tokens'].map(swh.removeStopWords)\n",
    "    task['Lem'] = task['StopWordsOut'].map(lemmatizer)\n",
    "    task['WCount'] = task['Lem'].map(wordCount)\n",
    "    task['f3'] = task.WCount.map(lambda t: getSim(task.WCount.iloc[-1], t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createF4(task):\n",
    "    task['Tokens'] = task['Content'].map(tokenizer).map(normalizer)\n",
    "    swh = StopWordsHandler()\n",
    "    task['StopWordsOut'] = task['Tokens'].map(swh.removeStopWords)\n",
    "    task['WCount'] = task['StopWordsOut'].map(wordCount)\n",
    "    task['f4'] = task.WCount.map(lambda t: getSim(task.WCount.iloc[-1], t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "corpusDF = readCorpusData(path)\n",
    "corpusDF = corpusDF[['File', 'Category','Content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "taska = corpusDF[corpusDF.File.str.contains('taska.txt')].fillna(0)\n",
    "taskb = corpusDF[corpusDF.File.str.contains('taskb.txt')].fillna(0)\n",
    "taskc = corpusDF[corpusDF.File.str.contains('taskc.txt')].fillna(0)\n",
    "taskd = corpusDF[corpusDF.File.str.contains('taskd.txt')].fillna(0)\n",
    "taske = corpusDF[corpusDF.File.str.contains('taske.txt')].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "createF1(taska)\n",
    "createF1(taskb)\n",
    "createF1(taskc)\n",
    "createF1(taskd)\n",
    "createF1(taske)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "createF2(taska)\n",
    "createF2(taskb)\n",
    "createF2(taskc)\n",
    "createF2(taskd)\n",
    "createF2(taske)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "createF3(taska)\n",
    "createF3(taskb)\n",
    "createF3(taskc)\n",
    "createF3(taskd)\n",
    "createF3(taske)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "createF4(taska)\n",
    "createF4(taskb)\n",
    "createF4(taskc)\n",
    "createF4(taskd)\n",
    "createF4(taske)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "taska.drop(taska.index[-1], inplace=True)\n",
    "taskb.drop(taskb.index[-1], inplace=True)\n",
    "taskc.drop(taskc.index[-1], inplace=True)\n",
    "taskd.drop(taskd.index[-1], inplace=True)\n",
    "taske.drop(taske.index[-1], inplace=True)\n",
    "\n",
    "frames = [taska, taskb, taskc, taskd, taske]\n",
    "\n",
    "df = pd.concat(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusão:\n",
      "[[0 3 1 1]\n",
      " [0 3 1 1]\n",
      " [0 0 0 0]\n",
      " [0 0 1 8]]\n",
      "\n",
      "Acurácia da classificação: 57.89473684210527%\n",
      "Resumo da classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cut       0.00      0.00      0.00         5\n",
      "       heavy       0.50      0.60      0.55         5\n",
      "       light       0.00      0.00      0.00         0\n",
      "         non       0.80      0.89      0.84         9\n",
      "\n",
      "   micro avg       0.58      0.58      0.58        19\n",
      "   macro avg       0.33      0.37      0.35        19\n",
      "weighted avg       0.51      0.58      0.54        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\renan.baisso\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\renan.baisso\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\renan.baisso\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\renan.baisso\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\renan.baisso\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\renan.baisso\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\renan.baisso\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "features = ['f1', 'f2', 'f3', 'f4']\n",
    "target = ['Category']\n",
    "#X features que utilizaremos para treinar o modelo\n",
    "X = df[features] \n",
    "\n",
    "#Y variável resposta, no caso nível de plágio\n",
    "y = df[target]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2) \n",
    "\n",
    "gauss = GaussianNB()\n",
    "\n",
    "gauss.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gauss.predict(X_test)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('Matriz de confusão:')\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "print(f'Acurácia da classificação: {accuracy*100}%')\n",
    "print('Resumo da classificação:')\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
